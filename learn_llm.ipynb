{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Mistral based models\n",
    "\n",
    "Adapted from: https://colab.research.google.com/github/brevdev/notebooks/blob/main/mixtral-finetune-own-data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from   datetime                import datetime\n",
    "from   pathlib                 import Path\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   transformers            import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from   peft                    import prepare_model_for_kbit_training\n",
    "from   peft                    import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "from   accelerate                                         import FullyShardedDataParallelPlugin, Accelerator\n",
    "from   torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset, shuffle it and perform the train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_dataset = []\n",
    "cnt         = 0\n",
    "with open('dataset.jsonl') as f:\n",
    "    lrn_dataset = [json.loads(l) for l in f]\n",
    "\n",
    "random.shuffle(lrn_dataset)\n",
    "train_d, test_d = train_test_split(lrn_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Identify if the text provided by user is related to Fablabs and the techniques and skills related or have no relevance to Fablabs, \"       \\\n",
    "              + \"provide the answer entirely in a json format containing a key \\\"text\\\" key containing the text under analysis and a \\\"fablab\\\" key \"       \\\n",
    "              + \"containing 1 if the text is related to Fablabs 0 otherwise\"\n",
    "\n",
    "verif_text    = \"DigiCon is an interactive, creative, inspiring event exploring how anyone can change the world using the new tools of digital fabrication.  In its 8th year, the 2019 DigiCon will focus on Digital Badges, a platform for verifying skills and accomplishments through the North American Digital Fabrication Alliance\"\n",
    "\n",
    "\n",
    "eval_prompt   = f\"<|system|>\\n{system_prompt}\\n<|user|>\\n{verif_text}</s>\\n<|assistant|>\\n\"\n",
    "verif_dict    = {'text': verif_text, 'fablab':1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Identify if the text provided by user is related to Fablabs and the techniques and skills related or have no relevance to Fablabs, provide the answer entirely in a json format containing a key \"text\" key containing the text under analysis and a \"fablab\" key containing 1 if the text is related to Fablabs 0 otherwise\n",
      "<|user|>\n",
      "DigiCon is an interactive, creative, inspiring event exploring how anyone can change the world using the new tools of digital fabrication.  In its 8th year, the 2019 DigiCon will focus on Digital Badges, a platform for verifying skills and accomplishments through the North American Digital Fabrication Alliance</s>\n",
      "<|assistant|>\n",
      " {\"text\": \"DigiCon is an interactive, creative, inspiring event exploring how anyone can change the world using the new tools of digital fabrication.  In its 8th year, the 2019 DigiCon will focus on Digital Badges, a platform for verifying skills and accomplishments through the North American Digital Fabrication Alliance\", \"fablab\": 1 } \n"
     ]
    }
   ],
   "source": [
    "base_model_id   = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "project         = \"fablab_finetune\"\n",
    "base_model_name = base_model_id.split('/')[-1]\n",
    "run_name        = base_model_name + \"_\" + project\n",
    "output_dir      = \"./\" + run_name\n",
    "output_dir_pth  = Path(output_dir)\n",
    "\n",
    "bnb_config    = BitsAndBytesConfig(\n",
    "    load_in_4bit              = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type       = \"nf4\",\n",
    "    bnb_4bit_compute_dtype    = torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer     = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side              = \"left\",\n",
    "    add_eos_token             = True,\n",
    "    add_bos_token             = True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def formatting_func(example):\n",
    "    text = f\"<|system|>\\n{system_prompt}\\n<|user|>\\n{example['text']}</s>\\n<|assistant|>\\n {{\\\"text\\\": \\\"{example['text']}\\\", \\\"fablab\\\": {1 if example['fablab'] == 1 else 0} }} \"\n",
    "    return text\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))\n",
    "\n",
    "\n",
    "print(formatting_func(verif_dict))     #test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_train_d = train_d.map(generate_and_tokenize_prompt)\n",
    "#tokenized_test_d  = test_d.map(generate_and_tokenize_prompt)\n",
    "\n",
    "tokenized_train_d = list(map(generate_and_tokenize_prompt, train_d))\n",
    "tokenized_test_d  = list(map(generate_and_tokenize_prompt, test_d ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd37f84b8cea42e0ab1ed5391ee5414c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_lengths(tokenize_train_dataset, tokenized_test_d):\n",
    "    lengths  = [len(x['input_ids']) for x in tokenized_train_d]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_test_d ]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_d, tokenized_test_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024 # This was an appropriate max length for my dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-init the tokenizer so it doesn't add padding or eos token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/transformers/generation/utils.py:1547: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Identify if the text provided by user is related to Fablabs and the techniques and skills related or have no relevance to Fablabs, provide the answer entirely in a json format containing a key \"text\" key containing the text under analysis and a \"fablab\" key containing 1 if the text is related to Fablabs 0 otherwise\n",
      "<|user|>\n",
      "DigiCon is an interactive, creative, inspiring event exploring how anyone can change the world using the new tools of digital fabrication.  In its 8th year, the 2019 DigiCon will focus on Digital Badges, a platform for verifying skills and accomplishments through the North American Digital Fabrication Alliance \n",
      "<|assistant|>\n",
      "{\n",
      "  \"text\": {\n",
      "    \"value\": \"DigiCon is an interactive, creative, inspiring event exploring how anyone can change the world using the new tools of digital fabrication.  In its 8th year, the 2019 DigiCon will focus on Digital Badges, a platform for verifying skills and accomplishments through the North American Digital Fabrication Alliance\"\n",
      "  },\n",
      "  \"fablab\": {\n",
      "    \"value\": 1\n",
      "  }\n",
      "}\n",
      "\n",
      "Explanation:\n",
      "Fablabs are physical spaces that provide access to digital fabrication tools such as 3D printers, laser cutters, CNC machines, etc. The text mentions \"digital fabrication\", which refers to the process of creating physical objects from digital designs using various technologies like 3D printing, CNC machining, etc. Therefore, it's safe to say that this text is related to Fablabs since these labs offer access to such equipment. Additionally, the fact that DigiCon focuses on Digital Badges, which is a platform for verifying skills and accomplishments related to digital fabrication, further reinforces the connection between this text and Fablabs.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param        = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(\n",
      "        in_features=4096, out_features=32000, bias=False\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r              = 32,\n",
    "    lora_alpha     = 64,\n",
    "    target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias           = \"none\",\n",
    "    lora_dropout   = 0.05,  # Conventional\n",
    "    task_type      = \"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config       = FullStateDictConfig(     offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config = FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)\n",
    "model       = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 75 steps\n"
     ]
    }
   ],
   "source": [
    "resume_from_ckp = False\n",
    "warm_st         = 1\n",
    "train_passes    = 2\n",
    "steps           = warm_st + len(tokenized_train_d) * train_passes\n",
    "\n",
    "print(f\"Training for {steps} steps\")\n",
    "trainer      = transformers.Trainer(\n",
    "                model         = model,\n",
    "                train_dataset = tokenized_train_d,\n",
    "                eval_dataset  = tokenized_test_d,\n",
    "                args          = transformers.TrainingArguments(\n",
    "                    output_dir                  = output_dir,\n",
    "                    warmup_steps                = warm_st,\n",
    "                    per_device_train_batch_size = 2,\n",
    "                    gradient_accumulation_steps = 1,\n",
    "                    max_steps                   = steps,\n",
    "                    learning_rate               = 1.0e-5,             # Want a small lr for finetuning\n",
    "                    bf16                        = True,\n",
    "                    optim                       = \"paged_adamw_8bit\",\n",
    "                    logging_steps               = 5,                  # When to start reporting loss\n",
    "                    logging_dir                 = \"./logs\",           # Directory for storing logs\n",
    "                    save_strategy               = \"steps\",            # Save the model checkpoint every logging step\n",
    "                    save_steps                  = 5,                  # Save checkpoints every 50 steps\n",
    "                    evaluation_strategy         = \"steps\",            # Evaluate the model every logging step\n",
    "                    eval_steps                  = 5,                  # Evaluate and save checkpoints every 50 steps\n",
    "                    do_eval                     = True,               # Perform evaluation at the end of training\n",
    "                    #report_to                   = \"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "                    run_name                    = f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting old checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4336143aa64fc690eed0e277662114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/torch/utils/checkpoint.py:461: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4899, 'learning_rate': 9.45945945945946e-06, 'epoch': 0.26}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c6ac46899b471c85ab18c285165875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3040425777435303, 'eval_runtime': 1.839, 'eval_samples_per_second': 7.069, 'eval_steps_per_second': 1.088, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/torch/utils/checkpoint.py:461: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2588, 'learning_rate': 8.783783783783785e-06, 'epoch': 0.53}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59272c5e02b546e6856d730b9ffc9dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1751329898834229, 'eval_runtime': 1.8285, 'eval_samples_per_second': 7.11, 'eval_steps_per_second': 1.094, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/torch/utils/checkpoint.py:461: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1588, 'learning_rate': 8.108108108108109e-06, 'epoch': 0.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892c66745de645faa12fca6cc28f4e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0848262310028076, 'eval_runtime': 1.8644, 'eval_samples_per_second': 6.973, 'eval_steps_per_second': 1.073, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/torch/utils/checkpoint.py:461: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1307, 'learning_rate': 7.4324324324324324e-06, 'epoch': 1.05}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08515ae8957441daa44332f22c192126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9888514876365662, 'eval_runtime': 1.8428, 'eval_samples_per_second': 7.054, 'eval_steps_per_second': 1.085, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/torch/utils/checkpoint.py:461: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9121, 'learning_rate': 6.7567567567567575e-06, 'epoch': 1.32}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea03feae232f42939c43d91c9d8e8bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8986876606941223, 'eval_runtime': 1.8597, 'eval_samples_per_second': 6.991, 'eval_steps_per_second': 1.075, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/torch/utils/checkpoint.py:461: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7945, 'learning_rate': 6.081081081081082e-06, 'epoch': 1.58}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d41beff76040a2aef63ed4e419fae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8429532647132874, 'eval_runtime': 1.8488, 'eval_samples_per_second': 7.032, 'eval_steps_per_second': 1.082, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/torch/utils/checkpoint.py:461: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7002, 'learning_rate': 5.405405405405406e-06, 'epoch': 1.84}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6852e0da604a128716fc4f1f662111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8161913156509399, 'eval_runtime': 1.8381, 'eval_samples_per_second': 7.072, 'eval_steps_per_second': 1.088, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/torch/utils/checkpoint.py:461: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7418, 'learning_rate': 4.72972972972973e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d7a50f6c2b4518a95c979f24c4d064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8041848540306091, 'eval_runtime': 1.8445, 'eval_samples_per_second': 7.048, 'eval_steps_per_second': 1.084, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/torch/utils/checkpoint.py:461: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6424, 'learning_rate': 4.0540540540540545e-06, 'epoch': 2.37}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63215e7a70b64f6787e28363fc2ba7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8015544414520264, 'eval_runtime': 1.8454, 'eval_samples_per_second': 7.045, 'eval_steps_per_second': 1.084, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/torch/utils/checkpoint.py:461: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7166, 'learning_rate': 3.3783783783783788e-06, 'epoch': 2.63}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f79da1a49d451094fbd0766b0f01a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8011030554771423, 'eval_runtime': 1.8663, 'eval_samples_per_second': 6.966, 'eval_steps_per_second': 1.072, 'epoch': 2.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/torch/utils/checkpoint.py:461: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7317, 'learning_rate': 2.702702702702703e-06, 'epoch': 2.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8ccc04135448af8881320b89c742a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7999594211578369, 'eval_runtime': 1.8658, 'eval_samples_per_second': 6.967, 'eval_steps_per_second': 1.072, 'epoch': 2.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/torch/utils/checkpoint.py:461: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6727, 'learning_rate': 2.0270270270270273e-06, 'epoch': 3.16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb06f7662454efcb02a87925c106d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7995823621749878, 'eval_runtime': 1.8657, 'eval_samples_per_second': 6.968, 'eval_steps_per_second': 1.072, 'epoch': 3.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/torch/utils/checkpoint.py:461: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6044, 'learning_rate': 1.3513513513513515e-06, 'epoch': 3.42}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f61a8aff8942f18dc03500bb164f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.798859715461731, 'eval_runtime': 1.8657, 'eval_samples_per_second': 6.968, 'eval_steps_per_second': 1.072, 'epoch': 3.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/torch/utils/checkpoint.py:461: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6472, 'learning_rate': 6.756756756756758e-07, 'epoch': 3.68}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b2b1c293074e05963998c9e1945792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7981175780296326, 'eval_runtime': 1.8648, 'eval_samples_per_second': 6.971, 'eval_steps_per_second': 1.073, 'epoch': 3.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/torch/utils/checkpoint.py:461: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6178, 'learning_rate': 0.0, 'epoch': 3.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5171a7e8b3c42ef8061f941d7a8ad2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.798119843006134, 'eval_runtime': 1.8629, 'eval_samples_per_second': 6.978, 'eval_steps_per_second': 1.074, 'epoch': 3.95}\n",
      "{'train_runtime': 95.6621, 'train_samples_per_second': 1.568, 'train_steps_per_second': 0.784, 'train_loss': 0.8546417077382406, 'epoch': 3.95}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=0.8546417077382406, metrics={'train_runtime': 95.6621, 'train_samples_per_second': 1.568, 'train_steps_per_second': 0.784, 'train_loss': 0.8546417077382406, 'epoch': 3.95})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if resume_from_ckp:\n",
    "    if output_dir_pth.exists():\n",
    "        print(\"Loading from checkpoint\")\n",
    "        trainer.train(resume_from_checkpoint=output_dir_pth)\n",
    "else:\n",
    "    if output_dir_pth.exists():\n",
    "        print(\"Deleting old checkpoint\")\n",
    "        #output_dir_pth.rmdir()\n",
    "        shutil.rmtree(output_dir_pth, ignore_errors=True)\n",
    "        \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b79cdac6dbb4e149d5276fd82588f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,                     # same as before\n",
    "    quantization_config = bnb_config,  # Same quantization config as before\n",
    "    device_map          = \"auto\",\n",
    "    trust_remote_code   = True,\n",
    "    #use_auth_token=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = PeftModel.from_pretrained(base_model, run_name + \"/checkpoint-50\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Identify if the text provided by user is related to Fablabs and the techniques and skills related or have no relevance to Fablabs, provide the answer entirely in a json format containing a key \"text\" key containing the text under analysis and a \"fablab\" key containing 1 if the text is related to Fablabs 0 otherwise\n",
      "<|user|>\n",
      "DigiCon is an interactive, creative, inspiring event exploring how anyone can change the world using the new tools of digital fabrication.  In its 8th year, the 2019 DigiCon will focus on Digital Badges, a platform for verifying skills and accomplishments through the North American Digital Fabrication Alliance</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(eval_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/ab311/lib/python3.11/site-packages/transformers/generation/utils.py:1547: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Identify if the text provided by user is related to Fablabs and the techniques and skills related or have no relevance to Fablabs, provide the answer entirely in a json format containing a key \"text\" key containing the text under analysis and a \"fablab\" key containing 1 if the text is related to Fablabs 0 otherwise\n",
      "<|user|>\n",
      "DigiCon is an interactive, creative, inspiring event exploring how anyone can change the world using the new tools of digital fabrication.  In its 8th year, the 2019 DigiCon will focus on Digital Badges, a platform for verifying skills and accomplishments through the North American Digital Fabrication Alliance \n",
      "<|assistant|>\n",
      " {\"text\": \"DigiCon is an interactive, creative, inspiring event exploring how anyone can change the world using the new tools of digital fabrication.  In its 8th year, the 2019 DigiCon will focus on Digital Badges, a platform for verifying skills and accomplishments through the North American Digital Fabrication Alliance \", \"fablab\": 1 } \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
